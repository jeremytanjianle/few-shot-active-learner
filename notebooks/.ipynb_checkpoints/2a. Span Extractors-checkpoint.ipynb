{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fe06e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('albert-base-v2', use_fast=False)\n",
    "encoder = AutoModel.from_pretrained('albert-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d1962",
   "metadata": {},
   "source": [
    "### Prepare document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44640634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Data_Handler, Doc_Span\n",
    "\n",
    "handler = Data_Handler()\n",
    "\n",
    "text = 'We believe TEMP.Mana, a Chinese cyber espionage group, is linked to infrastructure spoofing domains of at least two U.S. chemical manufacturers. Similar activity suspected of being tied to TEMP.Mana reinforces the risk to the chemical sector and related industries.'\n",
    "# text = \"During a compromise of a hospitality organization, which we attribute to FIN7, the attackers deployed various malicious payloads, including CARBANAK, BABYMETAL, and the PILLOWMINT point-of-sale (POS) malware. Notably, in this incident, FIN7 leveraged a seemingly compromised email account belonging one of the victim's suppliers to send a password-protected GRIFFON document to a restaurant manager. FIN7's shift to engaging in third-party compromise attacks marks a significant evolution in the group's operations and the manner in which organizations need to protect themselves against their attacks.\"\n",
    "text = \"GREF Team is an innovative China-nexus cyber espionage operator that focuses on targeting energy, video game development studios, and high-tech companies in the U.S., South Korea, the Netherlands, Italy, France, and Japan. The group leverages a large library of malware tools, compromised digital certificates, and exploits. Besides being linked to a common supplier shared with other Chinese cyber espionage groups, GREF Team is believed to distribute its tools to other distinct China-based actors.\"\n",
    "# text = \"In 1905, a year sometimes described as his annus mirabilis ('miracle year'), Einstein published four groundbreaking papers.[12] These outlined the theory of the photoelectric effect, explained Brownian motion, introduced special relativity, and demonstrated mass-energy equivalence. Einstein thought that the laws of classical mechanics could no longer be reconciled with those of the electromagnetic field, which led him to develop his special theory of relativity. He then extended the theory to gravitational fields; he published a paper on general relativity in 1916, introducing his theory of gravitation. In 1917, he applied the general theory of relativity to model the structure of the universe.\"\n",
    "# text = \"In May 2019, FireEye devices detected and blocked a spear-phishing campaign likely targeting Ukrainian government entities distributing the ARMEDCLOUD malware attributed to TEMP.Armageddon. Additional samples and related infrastructure were subsequently uncovered. This newest version of ARMEDCLOUD contains anti-forensic features.\"\n",
    "# text = 'A Chinese threat group was using hacking tools developed by the NSA more than a year before Shadow Brokers leaked them in April 2017, tools that were later used in highly destructive attacks such as the WannaCry ransomware\\xa0campaign from May 2017.\\n The Buckeye threat group (also known to researchers as Gothic Panda,\\xa0TG-0110, UPS, and APT3) has been active since at least 2010, it is credited by experts for running\\xa0Operation Clandestine Fox, Operation Clandestine Wolf, and Operation Double Tap\\xa0[1, 2, 3], and for mainly attacking U.S. entities with a sudden switch to Hong Kong targets back in 2015.\\n The indictment of three APT3 members by the U.S. government in November 2017 is the thing that really brought the group in the spotlight, with the three Chinese hackers being accused of infiltrating the computing systems of\\xa0Moody’s Analytics, Siemens, and Trimble\\xa0between 2011 and May 2017.\\n As unearthed by Symantec, the Chinese-backed Buckeye was using NSA hacking tools 13 months before they were leaked by Shadow Brokers—the hacking group who stole them—in April 2017, together with a \"previously unknown Windows zero-day vulnerability that Symantec discovered (which has since been patched by Microsoft).\"\\n\\n Starting with March 2016,\\xa0the NSA\\xa0DoublePulsar backdoor was detected\\xa0as part of Buckeye campaigns, while being dropped with the help of the Bemstour Trojan, a malware dropper specifically created by the group to deliver the NSA malware payload.\\n Symantec discovered that\\xa0the variant used by Buckeye during their attacks was newer than the one leaked by Shadow Brokers, with an extra layer of obfuscation which might indicate that the Chinese hackers customized it before deployment on their victims\\' systems.\\n'\n",
    "text = \"The activity of the advanced hacker group the researchers call Silence has increased significantly over the past year. Victims in the financial sector are scattered across more than 30 countries and financial losses have quintupled.\\n The group started timidly in 2016, learning the ropes by following the path beaten by other hackers. Since then, it managed to steal at least $4.2 million, initially from banks in the former Soviet Union, then from victims in Europe, Latin America, Africa, and Asia.\\n Researchers at Group-IB, Singapore-based cybersecurity company specializing in attack prevention, tracked Silence early on and judged its members to be familiar with white-hat security activity.\\n A report last year\\xa0details the roles of Silence hackers, their skills, failures, and successful bank heists\"\n",
    "text = \"Security researchers have discovered an ongoing cryptojacking campaign which infects unpatched computers of businesses from all over the world with XMRig Monero miners using Equation group's leaked exploit toolkit.\\n The cybercriminals behind this cryptomining campaign use the NSA-developed EternalBlue and EternalChampion SMB exploits to compromise vulnerable Windows computers, exploits which were leaked by the Shadow Brokers hacker group in April 2017.\\n While Microsoft patched the security flaws these tools abused to break into Windows machines [1, 2, 3], there are still a lot of exposed computers because they haven't been updated to newer OS versions not being impacted by these very dangerous vulnerabilities.\\n The campaign's targets\\n \\\"The campaign seems to be widespread, with targets located in all regions of the world. Countries with large populations such as China and India also had the most number of organizations being targeted,\\\" say Trend Micro's researchers, the ones who unearthed this ongoing cryptojacking campaign targeting companies from all over the world.\\n\"\n",
    "text = 'The Buhtrap hacking group has switched its targets from Rusian financial businesses and institutions since December 2015 when it moved into cyber-espionage operations, culminating with the use of a recently patched Windows zero-day during June 2019.\\n The\\xa0Windows local privilege escalation 0-day vulnerability tracked as\\xa0CVE-2019-1132\\xa0and abused by Buhtrap as part of its attacks was fixed by Microsoft during this month\\'s Patch Tuesday and it allowed the cyber-crime group to run\\xa0arbitrary code in kernel mode after successful exploitation.\\n Even though actively targeting banking clients since 2014,\\xa0Buhtrap\\'s attacks were only detected one year later, in 2015, when it started going after more high-profile victims like financial institutions according to Group-IB and ESET researchers.\\n \"From August 2015 to February 2016 Buhtrap\\xa0managed to conduct 13 successful attacks against\\xa0Russian banks for a total amount of 1.8 billion\\xa0rubles ($25.7 mln),\" says a Group-IB report.\\n The Windows\\xa0zero-day exploited by Buhtrap\\nESET\\xa0researchers were able to observe how the hacker group\\'s \"toolset has been expanded with malware used to conduct espionage in Eastern Europe and Central Asia\" in multiple\\xa0targeted campaigns.\\n Buhtrap\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\'s\\xa0zero-day vulnerability exploit was used during June 2019 in an attack against a governmental institution and it is designed to\\xa0abuse \"a NULL pointer dereference in the win32k.sys component\" on computers running older Windows versions.\\n'\n",
    "text = 'According to documents added to an amended complaint filed on January 17, a Russian intelligence-coordinated phishing attack allegedly targeted the Democratic National Committee (DNC) just a few days after the 2018\\xa0midterms.\\n Moreover, as detailed in the court documents, \"On November 14, 2018, dozens of DNC email addresses were targeted in a spear-phishing campaign, although there is no evidence that the attack was successful.\"\\n The documents were filed as part of a lawsuit against Russia\\'s government, as well as\\xa0the Trump campaign, for an alleged hack which led to a\\xa0trove of internal DNC emails being stolen and disclosed during 2016.\\n As revealed by the DNC, multiple links are connecting the actor behind the November phishing attack with a Russian hacker group known as\\xa0Cozy Bear (also classified as APT29, Office Monkeys, CozyCar, The Dukes, CozyDuke, or Grizzly Steppe).\\n Cozy Bear connected to attacks against U.S. targets in 2014\\n Evidence found by Kaspersky\\'s\\xa0Kurt Baumgartner and\\xa0CostiRaiu\\xa0back in 2015 shows that Cozy Bear has been previously connected to attacks targeting both commercial and government entities from Germany, South Korea, Uzbekistan, and the USA, including the White House and the US State Department in 2014.\\n'\n",
    "text = \"The APT10 group (also known as Red Tears) is responsible and is linked to the North Korean nexus, and is charged with discharging the CryptoMix virus, retrieving money, phone numbers, details and credentials\"\n",
    "doc = handler.process_sentence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f37112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'APT10 group'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Doc_Span(doc, (1,2)).span_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b6cda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "piece_id = tokenizer.encode(doc.subword_tokens, return_tensors='pt')\n",
    "encodings = encoder(piece_id)['last_hidden_state']\n",
    "encodings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1c275",
   "metadata": {},
   "source": [
    "Prepare the spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94554a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueeze = lambda x: torch.unsqueeze(x, 0)\n",
    "tensorify_tuple = lambda span_tuple: unsqueeze( unsqueeze( torch.tensor(span_tuple) ) )\n",
    "\n",
    "span_tuple = (1,2)\n",
    "torch_span = tensorify_tuple(span_tuple)\n",
    "torch_span"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ff1920",
   "metadata": {},
   "source": [
    "### Allennlp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ed293c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.data.dataset_readers.dataset_utils.span_utils import enumerate_spans\n",
    "span_width_tuples = enumerate_spans(doc, max_span_width=5)\n",
    "\n",
    "from allennlp.modules.span_extractors.self_attentive_span_extractor import SelfAttentiveSpanExtractor\n",
    "span_extractor = SelfAttentiveSpanExtractor(768)\n",
    "\n",
    "span_representation = span_extractor(encodings, torch_span)\n",
    "span_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e8d2313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_convert_head_mask_to_5d',\n",
       " '_expand_inputs_for_generation',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_decoder_start_token_id',\n",
       " '_get_logits_processor',\n",
       " '_get_logits_warper',\n",
       " '_get_name',\n",
       " '_get_pad_token_id',\n",
       " '_get_resized_embeddings',\n",
       " '_get_resized_lm_head',\n",
       " '_get_stopping_criteria',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_sequence_length_for_generation',\n",
       " '_init_weights',\n",
       " '_is_full_backward_hook',\n",
       " '_keys_to_ignore_on_load_missing',\n",
       " '_keys_to_ignore_on_load_unexpected',\n",
       " '_keys_to_ignore_on_save',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_prepare_attention_mask_for_generation',\n",
       " '_prepare_decoder_input_ids_for_generation',\n",
       " '_prepare_encoder_decoder_kwargs_for_generation',\n",
       " '_prepare_input_ids_for_generation',\n",
       " '_prune_heads',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_reorder_cache',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_update_model_kwargs_for_generation',\n",
       " '_update_seq_length_for_generation',\n",
       " '_version',\n",
       " 'add_memory_hooks',\n",
       " 'add_module',\n",
       " 'adjust_logits_during_generation',\n",
       " 'apply',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'beam_sample',\n",
       " 'beam_search',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'config',\n",
       " 'config_class',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'device',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'embeddings',\n",
       " 'encoder',\n",
       " 'estimate_tokens',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'floating_point_ops',\n",
       " 'forward',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_head_mask',\n",
       " 'get_input_embeddings',\n",
       " 'get_output_embeddings',\n",
       " 'greedy_search',\n",
       " 'group_beam_search',\n",
       " 'half',\n",
       " 'init_weights',\n",
       " 'invert_attention_mask',\n",
       " 'is_parallelizable',\n",
       " 'load_state_dict',\n",
       " 'load_tf_weights',\n",
       " 'modules',\n",
       " 'name_or_path',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_parameters',\n",
       " 'parameters',\n",
       " 'pooler',\n",
       " 'pooler_activation',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'prune_heads',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_token_embeddings',\n",
       " 'sample',\n",
       " 'save_pretrained',\n",
       " 'set_input_embeddings',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4915dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.config.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd4515",
   "metadata": {},
   "source": [
    "### Sequence tensor (batch_size, seq_length, embed_size)\n",
    "### Span_indices (batch_size, num_spans, 2) \n",
    "span indices are inclusive start and end indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef108a8",
   "metadata": {},
   "source": [
    "### Transformers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c483fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"The APT10 group (also known as Temp.Mana) is responsible and is linked to the North Korean nexus, and is charged with discharging the CryptoMix virus, retrieving money, phone numbers, details and credentials\"\n",
    "\n",
    "doc = nlp(text)\n",
    "doc_tokens = [_.text for _ in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "04fa8732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from allennlp.modules.span_extractors.self_attentive_span_extractor import SelfAttentiveSpanExtractor\n",
    "from src.data import Doc_Tokens, Doc_Span\n",
    "\n",
    "\n",
    "class Dygie_Ent(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert_name = 'albert-base-v2', use_proj = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # encoder\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_name, use_fast=False)\n",
    "        self.encoder = AutoModel.from_pretrained(bert_name)\n",
    "        self.hidden_size = encoder.config.hidden_size\n",
    "\n",
    "        # scorer parameters\n",
    "        self.span_extractor = SelfAttentiveSpanExtractor(self.hidden_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(self.hidden_size, affine = False)\n",
    "        self.proj = nn.Sequential(nn.Linear(768,768), nn.Linear(768,768))\n",
    "        self.use_proj = use_proj\n",
    "\n",
    "        # supports\n",
    "        self.references = {}\n",
    "    \n",
    "    def encode_span(self, doc_tokens: Doc_Tokens, span_width_tuple: tuple, verbose=False):\n",
    "        \"\"\"\n",
    "        :param doc_tokens: Doc_Tokens object that contains the spans\n",
    "        :param span_width_tuple: tuple indicating start and end of spacy token spans\n",
    "        :return span_encoding: (span length x embed dimensions) torch array\n",
    "        \"\"\"\n",
    "        # get span \n",
    "        span_start, span_end = span_width_tuple # this is \n",
    "        span_start, span_end = doc_tokens.subword_idx[span_start], doc_tokens.subword_idx[span_end+1]-1\n",
    "        torch_span = tensorify_tuple( (span_start +1, span_end +1 ) ) # add to account for CLS token\n",
    "        \n",
    "        # encode the pieces and slice according to span\n",
    "        piece_id = self.tokenizer.encode(doc_tokens.subword_tokens, return_tensors='pt')\n",
    "        encodings = self.encoder(piece_id)['last_hidden_state']\n",
    "        span_representation = self.span_extractor(encodings, torch_span)\n",
    "\n",
    "        if verbose:\n",
    "            print(span_start)\n",
    "            print(span_end)\n",
    "            print(doc_tokens.subword_tokens[span_start: span_end+1]) # slice is exclusive, so +1\n",
    "            print(f\"start: {self.tokenizer.decode(piece_id[0,span_start+1])}\")\n",
    "            print(f\"end: {self.tokenizer.decode(piece_id[0,span_end+1 ])}\")\n",
    "        \n",
    "        if self.use_proj: span_representation = self.proj(span_representation)\n",
    "            \n",
    "        return span_representation\n",
    "    \n",
    "    \n",
    "    def encode_spans(self, doc_tokens: Doc_Tokens, span_width_tuples: list, verbose=False):\n",
    "        \"\"\"\n",
    "        :param doc_tokens: Doc_Tokens object that contains the spans\n",
    "        :param span_width_tuples: list of tuple indicating start and end of spacy token spans\n",
    "        :return span_encoding: (span length x embed dimensions) torch array\n",
    "        \"\"\"\n",
    "        # encode the pieces and slice according to span\n",
    "        piece_id = self.tokenizer.encode(doc_tokens.subword_tokens, return_tensors='pt')\n",
    "        encodings = self.encoder(piece_id)['last_hidden_state']\n",
    "        \n",
    "        # get ecodings of spans\n",
    "        torch_spans = []\n",
    "        for span_width_tuple in span_width_tuples:\n",
    "            span_start, span_end = span_width_tuple\n",
    "            # end +1 to get the index of end subword token\n",
    "            span_start, span_end = doc_tokens.subword_idx[span_start], doc_tokens.subword_idx[span_end+1]-1\n",
    "            # span starts and end +1 to account for inserted CLS token\n",
    "            torch_span = tensorify_tuple( (span_start +1, span_end +1 ) ) # add to account for CLS token\n",
    "            torch_spans.append(torch_span)\n",
    "            \n",
    "        torch_spans = torch.cat(torch_spans, axis=1)\n",
    "        span_representations = self.span_extractor(encodings, torch_spans)\n",
    "        \n",
    "        if self.use_proj: span_representations = self.proj(span_representations)\n",
    "        return span_representations\n",
    "    \n",
    "    def encoding_sim_score(self, query_encoding: torch.Tensor, reference_encoding: torch.Tensor):\n",
    "        sim = sim_matrix(query_encoding, reference_encoding)\n",
    "        max_vals = torch.mean( sim, axis=1 ) \n",
    "        max_vals += self.threshold_bias    \n",
    "        score = torch.mean( self.act(max_vals) )\n",
    "        # score = torch.sum( self.act(max_vals) )\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55fcc24",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5ea3fae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "['▁a', 'pt', '10', '▁group']\n",
      "start: a\n",
      "end: group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.6610e-02, -5.8114e-01,  1.0357e+00, -1.6544e+00,  2.3696e-01,\n",
       "           4.0819e-01, -8.1892e-01, -1.2327e-01,  3.8118e-01,  6.3529e-01,\n",
       "           7.0002e-01,  1.4037e+00, -1.9081e-01, -3.8178e-01,  4.7446e-01,\n",
       "          -5.1361e-01, -3.6922e-01, -3.0399e-01, -5.9721e-01, -3.0166e-01,\n",
       "          -1.7863e+00,  8.5856e-01, -2.7786e-01,  1.7922e+00, -6.5411e-01,\n",
       "           7.7932e-01, -2.0163e-01,  3.7931e-01,  4.5837e-01,  1.6876e+00,\n",
       "          -4.4428e-01, -6.3942e-01, -5.0874e-01,  5.1962e-02, -6.7953e+00,\n",
       "          -3.9944e-01,  9.3662e-02,  2.2824e-01, -3.3771e-01,  7.5844e-01,\n",
       "          -5.8771e-01,  2.1555e-01, -3.8541e-01, -1.3366e+00, -1.0976e+00,\n",
       "           2.9118e-01,  1.5475e-01,  7.6958e-01, -4.3306e-01,  1.2252e+00,\n",
       "           3.8254e-01, -1.1993e-01, -2.0134e-01, -1.0243e+00, -1.1249e+00,\n",
       "          -1.6410e-01, -4.2170e-01,  5.4595e-01, -1.0167e+00,  9.0729e-01,\n",
       "           5.1247e-01, -7.5741e-01, -3.1614e-01, -4.1748e-01,  2.3829e-01,\n",
       "          -5.6907e-01,  4.6262e-01, -2.9811e-01, -1.2755e+00, -1.0632e+00,\n",
       "          -4.2087e-01, -2.1196e-01, -3.7266e-02, -4.4255e-01,  1.5285e-01,\n",
       "           1.2369e+00, -4.7721e-01,  2.8052e-01, -5.8881e-01, -1.9642e-01,\n",
       "          -1.7526e-01, -4.0334e-01, -6.4122e-01,  1.0069e+00, -6.5026e-01,\n",
       "           1.2984e+00, -2.7047e-01, -7.8786e-01,  4.7121e-01,  3.0703e-01,\n",
       "          -1.1482e+00, -3.4791e-01,  7.9640e-01,  6.8078e-01,  4.3806e-01,\n",
       "           2.3160e-01,  1.0139e+00, -6.5048e-01, -3.8841e-01,  2.1275e-01,\n",
       "          -3.0948e-01,  3.9294e-01,  2.9133e-01,  1.7391e-01,  4.3719e-01,\n",
       "           3.1762e-03,  1.9281e-02,  2.3808e+00,  1.4667e-01, -1.1772e+00,\n",
       "          -1.2391e+00, -3.1016e-01,  1.0853e-01, -2.1132e-01, -2.4180e+00,\n",
       "           4.0788e-01,  1.4461e+00,  1.6123e-02,  1.9995e-01, -2.7152e-01,\n",
       "           7.6837e-01, -1.9131e-01,  4.4811e-01, -3.8200e-01,  1.2302e+00,\n",
       "          -4.3493e-01,  6.5388e-01,  1.5148e+00, -1.1043e-01,  4.3861e-01,\n",
       "           3.4479e-02, -1.9660e-01, -1.3731e+00, -1.0065e-02, -4.2137e-01,\n",
       "          -5.2654e-01,  1.1079e+00,  2.5502e-01, -2.8627e-01, -4.4799e-01,\n",
       "           6.5393e-02,  2.6699e-01, -1.2632e+00,  2.8979e-01,  1.4324e+00,\n",
       "           1.5710e+00, -5.5729e-01, -9.0625e-01, -4.4836e-01,  4.6454e-01,\n",
       "           7.3375e-01,  4.3238e-03, -5.8561e-01,  1.0712e+00,  6.8933e-01,\n",
       "          -1.5688e+00,  8.9956e-01,  6.4775e-01, -1.0125e-01,  4.3234e-01,\n",
       "          -6.3417e-02,  6.8901e-01,  5.3407e-01, -1.4740e+00,  1.2548e+00,\n",
       "           6.1779e-01,  1.3681e-01, -4.6248e-01, -7.9495e-01,  7.4297e-01,\n",
       "           1.8683e+00, -7.7435e-01, -1.2709e+00, -9.4797e-01,  7.8381e-01,\n",
       "          -1.7871e-01, -2.5244e+00, -5.6539e-03, -2.7585e-01, -1.4529e-01,\n",
       "          -7.4974e-01,  3.1978e-02, -2.6205e-01,  1.5429e+00,  1.0556e+00,\n",
       "           1.1898e+00,  9.1111e-01, -1.2638e+00, -3.5093e-01, -1.1445e+00,\n",
       "           4.0025e-01,  4.6582e-02, -6.2890e-01,  7.4858e-01, -1.0866e+00,\n",
       "           4.0571e-01, -1.2364e+00, -2.9664e-01, -4.3870e-01, -2.4615e-01,\n",
       "          -3.4830e-01,  6.2840e-01, -9.2033e-01,  3.3778e-02,  2.0674e+00,\n",
       "          -2.2371e-01,  6.9052e-01, -3.4568e-01, -4.4696e-02,  7.4025e-01,\n",
       "          -8.4499e-02,  4.4126e-01, -4.7277e-01, -1.9782e+00,  7.4801e-01,\n",
       "           7.2601e-01,  7.1301e-01,  9.6950e-01,  6.9070e-01,  5.4668e-01,\n",
       "          -6.4922e-01, -3.3374e-01,  7.8127e-01, -3.5368e-02, -5.9291e-01,\n",
       "          -6.7089e-01,  3.6158e-01, -1.7142e-01, -2.1318e-01, -1.1448e+00,\n",
       "          -1.1738e-01, -6.4849e-01,  8.6160e-01, -1.3531e+00, -6.2027e-01,\n",
       "           3.2238e-01, -5.8886e-01,  6.0855e-01, -7.0483e-01,  1.0693e+00,\n",
       "          -4.8223e-01, -5.4321e-01,  1.9591e-01, -5.0917e-01, -5.1753e-01,\n",
       "           4.8249e-01, -9.6564e-02,  1.4102e-01, -1.4176e+00,  1.4789e-01,\n",
       "          -9.2358e-01,  8.3402e-01,  5.2056e-01,  8.0989e-01,  5.9412e-01,\n",
       "          -6.8085e-01,  6.8190e-01,  7.0882e-01, -5.1262e-01, -1.8368e+00,\n",
       "          -1.4748e+00,  7.7000e-01, -4.2366e-01, -2.4416e-02,  1.3298e-01,\n",
       "          -2.4585e-01,  2.4961e-01, -1.8070e-01, -8.7917e-01, -8.4504e-01,\n",
       "           8.1765e-01, -5.5215e-01,  5.8922e-01, -4.8516e-01, -1.5070e-01,\n",
       "           8.1253e-01,  8.0302e-01,  9.3268e-01,  9.4455e-01, -2.1089e-01,\n",
       "           4.5850e-01, -4.0413e-01, -4.7618e-01, -1.1527e-01, -5.1241e-01,\n",
       "          -2.4481e-01,  5.4681e-01, -6.7369e-01, -8.2398e-01,  4.0342e-01,\n",
       "          -7.8301e-01,  1.1435e+00,  8.9040e-01,  7.4022e-01, -3.3396e-01,\n",
       "          -7.2489e-01,  1.7860e-01,  5.0391e-01, -1.4156e+00,  1.5790e-01,\n",
       "           6.9450e-01,  9.9752e-01, -1.7081e-01,  6.0697e-03, -6.4345e-01,\n",
       "          -8.5652e-02, -1.2404e-01,  1.4181e-01, -5.3980e-01, -4.3894e-01,\n",
       "          -8.2895e-02, -5.0056e-01,  6.1880e-01, -6.7521e-02, -8.8344e-01,\n",
       "           6.4654e-01,  2.4830e-01, -3.3421e-01, -6.8389e-02, -1.4372e+00,\n",
       "           1.9046e-01, -1.8070e-01,  6.1420e-01, -2.8262e-01, -4.0478e-01,\n",
       "          -7.8979e-01, -5.1052e-01,  2.1245e-01,  1.2312e-01, -1.8479e-01,\n",
       "          -5.3370e-01,  3.0703e-01, -1.8715e+00,  6.8861e-01, -3.2068e-01,\n",
       "           1.1612e+00,  2.7341e-01, -7.7845e-01,  1.0365e+00, -4.0457e-01,\n",
       "          -3.3571e-01,  2.0310e-01,  5.4282e-01,  6.5322e-02, -1.4404e+00,\n",
       "           4.8269e-01, -4.8735e-01, -2.4588e-01, -6.4406e-01, -7.1758e-01,\n",
       "           3.7305e-01, -9.0019e-01, -2.8692e-01, -1.2028e+00,  1.2882e+00,\n",
       "           6.0403e-01,  5.0278e-01,  2.6071e-01, -3.4921e-02, -8.3669e-01,\n",
       "           1.0249e+00,  4.2115e-01,  1.1372e+00, -8.0339e-01, -8.0791e-01,\n",
       "           4.2101e-01,  1.8167e-01,  8.5919e-01, -5.0054e-01,  1.4496e+00,\n",
       "           1.2367e+00,  7.2598e-01, -5.9474e-01,  6.7568e-01, -4.6764e-02,\n",
       "           1.6240e-01,  1.1015e-01, -2.6803e-01,  2.1357e-01,  1.2229e+00,\n",
       "           2.1405e-01,  1.2550e+00,  4.2026e-01, -1.8064e-01,  1.2254e+00,\n",
       "          -3.9497e-01,  1.6223e+00,  5.4673e-01, -6.5049e-01,  2.2582e-01,\n",
       "          -3.4759e-01, -5.3009e-01,  4.8633e-01,  1.5310e-01, -6.4666e-01,\n",
       "          -5.1167e-01,  5.2179e-01, -3.9842e-01, -1.6399e+00,  1.3722e+00,\n",
       "          -6.8914e-01, -1.0688e+00,  1.7005e-01,  1.0088e+00,  6.3417e-01,\n",
       "           1.0502e+00, -6.4376e-01,  1.1591e+00,  2.5719e-01, -3.2960e-01,\n",
       "          -1.4811e+00,  5.4846e-01, -2.9695e-01, -9.4027e-01,  3.1208e-01,\n",
       "           8.0845e-01,  6.8720e-01, -9.4317e-01, -2.2668e-02,  3.6299e-01,\n",
       "           4.7465e-01,  1.3952e+00,  2.0228e-01,  3.6444e-01,  1.6279e+00,\n",
       "           2.4371e-01, -3.5575e-01, -1.1350e-01,  8.6922e-01, -3.1951e-02,\n",
       "           2.6909e-01,  2.8522e-01,  3.1835e-01,  1.4371e+00,  7.0746e-01,\n",
       "          -1.6423e-01, -5.3267e-01, -7.2019e-01,  4.8726e-01, -5.4123e-01,\n",
       "          -2.0185e+00, -5.8533e-01,  3.0225e-01,  9.1334e-01, -6.6334e-01,\n",
       "           2.6429e-01,  3.9734e-01, -2.9913e-01, -1.0377e-01, -8.8215e-01,\n",
       "          -3.7240e-01, -7.6045e-01, -1.3254e-03,  6.1956e-02, -3.3700e-01,\n",
       "           6.9995e-01,  1.6538e-01, -4.4134e-03, -8.8258e-02, -6.3962e-01,\n",
       "           1.0508e+00,  3.8541e-03, -4.3326e-01,  8.2692e-01,  1.3896e+00,\n",
       "           9.1966e-02,  5.2966e-01,  3.3594e-01, -1.6750e-01, -3.0495e-01,\n",
       "          -1.2198e+00, -1.9172e-01, -1.1877e+00,  6.6481e-01, -1.5195e-01,\n",
       "          -3.9387e-01,  2.3222e+00, -2.2406e-01, -1.6723e-01, -2.7683e-01,\n",
       "           1.7302e-01, -1.8688e-01, -2.1429e+00,  6.2553e-01, -6.6945e-01,\n",
       "          -6.1204e-01, -1.1040e+00,  4.9246e-01, -7.2877e-01, -1.6872e+00,\n",
       "          -6.1368e-01,  4.4065e-01,  5.9753e-01,  9.2652e-01, -1.1999e+00,\n",
       "          -6.5847e-01,  1.2695e+00, -9.1413e-01,  5.6984e-01, -3.4115e-01,\n",
       "          -2.3456e-01,  2.3751e-01, -1.2664e-01,  7.7867e-03,  7.6980e-01,\n",
       "           1.5446e+00, -1.4700e-01,  5.6720e-01,  6.4622e-01, -5.0738e-02,\n",
       "           5.4625e-01, -5.9782e-01, -1.5899e+00, -4.8912e-01, -7.5378e-01,\n",
       "          -9.7022e-01,  1.5656e-01, -8.1381e-02,  3.7186e-01, -2.9076e-01,\n",
       "           7.8310e-01,  2.9311e-01,  5.6898e-01, -5.0055e-01,  4.5504e-01,\n",
       "          -1.3711e+00,  3.3263e-01, -5.6775e-01, -1.2545e+00, -7.2290e-01,\n",
       "          -1.3788e+00, -9.0431e-01,  1.5559e-01, -9.2355e-01, -1.5881e+00,\n",
       "           1.2797e+00,  8.7642e-01, -6.0339e-02,  4.0081e-01, -4.3969e-01,\n",
       "          -1.4032e-01,  1.0350e+00,  6.2647e-01,  1.5248e+00,  2.0331e+00,\n",
       "           6.9906e-01,  7.2344e-01,  4.0310e-01, -8.3201e-01, -5.1698e-01,\n",
       "           3.8598e-01, -1.6035e-01,  4.0476e-01, -4.3099e-01, -6.0903e-01,\n",
       "           4.3264e-01,  6.8252e-03,  1.5684e+00, -3.6200e-01,  7.4619e-01,\n",
       "           8.2867e-01, -1.1388e+00, -5.6260e-01, -6.2633e-01,  2.6589e+00,\n",
       "           4.3556e-01, -4.7426e-01,  1.1106e-01, -3.5703e-01,  1.8460e-01,\n",
       "           1.5412e-02, -6.7791e-01, -9.8418e-01, -3.0801e-01,  2.5693e-02,\n",
       "          -6.5101e-01, -1.3340e+00,  9.1587e-01, -3.5958e-01, -4.6535e-01,\n",
       "          -2.0687e+00, -8.8509e-01,  1.0110e-01,  1.1165e-01,  2.6860e-01,\n",
       "           6.9954e-01,  3.8110e-01, -1.1119e+00,  1.2207e+00, -6.1920e-01,\n",
       "          -2.1792e-01, -1.2636e+00, -9.2182e-02,  1.8191e+00, -1.8705e-02,\n",
       "          -2.1003e-02, -5.6403e-02, -9.6337e-01, -6.9350e-02, -4.2455e-01,\n",
       "          -4.3601e-01, -7.9316e-01,  1.8789e-01, -8.4668e-01, -5.9694e-01,\n",
       "           6.9196e-01, -1.4711e+00, -5.4595e-01,  8.7041e-01, -1.9903e-01,\n",
       "          -6.8942e-01, -1.4308e+00, -1.2408e+00,  3.0022e-01, -3.5628e-02,\n",
       "           1.1138e-01,  3.1614e-01, -1.3659e+00,  6.6567e-01, -7.9269e-01,\n",
       "          -6.1275e-01, -2.6489e-03, -6.8346e-01, -6.6290e-02,  1.0004e-01,\n",
       "          -1.2047e-01,  3.2292e-01, -4.0159e-02,  4.1927e-01, -8.8842e-01,\n",
       "          -1.7717e+00, -1.7246e+00, -3.3017e-01, -7.4613e-01, -2.3449e-01,\n",
       "          -1.0264e+00, -6.7380e-01, -7.7966e-01,  1.1284e-01, -2.4481e-01,\n",
       "          -9.4050e-01, -2.6807e-01,  2.0633e-02, -5.0749e-01, -5.2224e-01,\n",
       "          -3.9113e-01,  1.0451e+00, -6.7307e-01,  6.1549e-01, -5.7643e-01,\n",
       "          -9.5185e-01, -6.3584e-01, -6.3962e-02, -6.5589e-01,  5.6413e-01,\n",
       "          -1.4757e-01,  1.6051e+00, -1.6187e+00,  8.4381e-02, -3.5668e-02,\n",
       "          -5.4006e-01, -6.6394e-01, -1.3896e-01,  6.3315e-02, -6.4775e-02,\n",
       "           1.2552e+00,  7.3472e-01, -8.2568e-01, -9.1523e-01, -5.5430e-01,\n",
       "          -1.7330e+00,  9.6225e-01, -5.5093e-01,  6.3177e-02,  5.0383e-01,\n",
       "           1.8995e+00, -2.8977e-01, -1.8340e-01, -2.4806e-01,  1.0569e+00,\n",
       "          -2.7355e-01, -1.8720e-01,  5.2746e-01,  3.3195e-01, -8.9113e-01,\n",
       "          -1.8406e+00, -5.2463e-01, -3.2259e-01,  1.3945e+00, -4.9071e-01,\n",
       "           5.7405e-01,  6.3641e-01, -5.5065e-02,  7.2234e-01, -7.5383e-01,\n",
       "           3.1313e-01,  2.9759e-02,  2.4042e-01, -2.4010e-01,  1.4192e+00,\n",
       "          -7.7854e-01, -7.7849e-01,  9.1113e-01, -9.8346e-02, -2.3458e-02,\n",
       "           3.4546e-01,  1.1446e+00, -1.2472e+00,  2.7165e-01, -3.1708e-01,\n",
       "           4.5074e-01,  8.3755e-01, -6.8570e-01,  2.5237e-01, -5.0228e-01,\n",
       "           7.8348e-01, -4.3554e-01, -3.5700e-01,  1.4886e+01, -6.3051e-01,\n",
       "           1.4289e-01, -2.1026e-01,  5.2906e-01,  5.8449e-01,  8.2363e-01,\n",
       "          -1.3843e-01,  1.7666e+00, -1.8506e-01, -9.3543e-01,  4.3125e-01,\n",
       "           9.0479e-01, -5.5323e-01,  2.1455e-01,  1.1559e+00, -2.2107e-01,\n",
       "          -2.4572e-01,  1.4355e-01, -1.9913e-01,  5.1754e-01,  1.1388e-01,\n",
       "           4.4551e-01,  4.4765e-01, -1.1083e-01, -1.0141e-01,  6.3844e-01,\n",
       "           9.1193e-01, -6.3487e-01,  2.1936e-01,  2.4266e-01, -6.7371e-01,\n",
       "          -3.3156e-01,  3.1598e-01,  7.5469e-01,  4.4774e-01, -8.7401e-01,\n",
       "          -5.0495e-01,  1.9648e-01, -5.1045e-01,  7.6662e-01, -1.0353e+00,\n",
       "           3.2420e-01, -7.6767e-01,  1.6612e+00, -6.8411e-01, -4.4334e-01,\n",
       "          -9.5980e-01,  1.6677e+00, -2.2884e-01]]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Dygie_Ent()\n",
    "model.encode_span(doc, (1,2), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d22367d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(4, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 768])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Dygie_Ent()\n",
    "span_encodings = model.encode_spans(doc, [(1,2), (4,5)], verbose=1)\n",
    "span_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e79bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d8556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100abb5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
