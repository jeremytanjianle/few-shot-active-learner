{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['single_entity.jsonl', 'v4.coref-separated.jsonl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.chdir('..')\n",
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'We believe TEMP.Mana, a Chinese cyber espionage group, is linked to infrastructure spoofing domains of at least two U.S. chemical manufacturers. Similar activity suspected of being tied to TEMP.Mana reinforces the risk to the chemical sector and related industries.',\n",
       " 'meta': {'source': 'fireeye-0-0'},\n",
       " 'spans': [{'start': 68,\n",
       "   'end': 99,\n",
       "   'label': 'ENT',\n",
       "   'token_start': 13,\n",
       "   'token_end': 16,\n",
       "   '_private_string': 'infrastructure spoofing domains'}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading from jsonlines file\n",
    "with open('data/single_entity.jsonl', 'rb') as f:\n",
    "    lines = f.readlines()\n",
    "lines = [json.loads(line.decode('utf-8')) for line in lines]\n",
    "line = lines[0]\n",
    "text = line['text']\n",
    "line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Sparta ent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.dataset_readers.dataset_utils.span_utils import enumerate_spans\n",
    "from src.model import Sparta_Ent\n",
    "from src.data import Data_Handler\n",
    "\n",
    "model = Sparta_Ent()\n",
    "handler = Data_Handler()\n",
    "doc = handler.process_sentence(text)\n",
    "\n",
    "span_tuples = enumerate_spans(doc.doc, max_span_width=7)\n",
    "span_encodings = model.encode_spans(doc, span_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁we', '▁believe', '▁', 'temp', '.', 'man', 'a', '▁', ',', '▁a']\n",
      "torch.Size([10, 768])\n"
     ]
    }
   ],
   "source": [
    "span_encoding = model.encode_span(doc, (0,4), verbose=True)\n",
    "print(span_encoding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `Doc_Span` and `Doc_Token` follow AllenNLP's inclusive spans instead of spacy's   exclusive spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "We believe TEMP.Mana, a"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "We believe TEMP.Mana, a"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.doc[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get reference encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_text = \"The APT10 group (also known as Red Tears) is responsible and is linked to the North Korean nexus, and is charged with discharging the CryptoMix virus, retrieving money, phone numbers, details and credentials\"\n",
    "ref_doc = handler.process_sentence(ref_text)\n",
    "reference_encoding = model.encode_span(ref_doc, (1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the span candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rank(model, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_scores = {span_tuple:model.encoding_sim_score(query_encoding, reference_encoding) \n",
    "               for span_tuple, query_encoding in zip(span_tuples,span_encodings)}\n",
    "sorted_span_scores = {k: v for k, v in \n",
    "                      sorted(span_scores.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 7): tensor(4.8290, grad_fn=<SumBackward0>),\n",
       " (0, 6): tensor(4.7705, grad_fn=<SumBackward0>),\n",
       " (2, 8): tensor(4.7536, grad_fn=<SumBackward0>),\n",
       " (2, 7): tensor(4.4192, grad_fn=<SumBackward0>),\n",
       " (0, 5): tensor(4.4024, grad_fn=<SumBackward0>),\n",
       " (1, 6): tensor(4.3962, grad_fn=<SumBackward0>),\n",
       " (31, 37): tensor(4.1353, grad_fn=<SumBackward0>),\n",
       " (29, 35): tensor(4.0843, grad_fn=<SumBackward0>),\n",
       " (30, 36): tensor(4.0653, grad_fn=<SumBackward0>),\n",
       " (0, 4): tensor(4.0287, grad_fn=<SumBackward0>),\n",
       " (1, 5): tensor(4.0281, grad_fn=<SumBackward0>),\n",
       " (2, 6): tensor(3.9864, grad_fn=<SumBackward0>),\n",
       " (28, 34): tensor(3.9664, grad_fn=<SumBackward0>),\n",
       " (27, 33): tensor(3.8204, grad_fn=<SumBackward0>),\n",
       " (29, 34): tensor(3.7729, grad_fn=<SumBackward0>),\n",
       " (31, 36): tensor(3.7723, grad_fn=<SumBackward0>),\n",
       " (30, 35): tensor(3.7508, grad_fn=<SumBackward0>),\n",
       " (26, 32): tensor(3.7170, grad_fn=<SumBackward0>),\n",
       " (1, 4): tensor(3.6543, grad_fn=<SumBackward0>),\n",
       " (19, 25): tensor(3.6371, grad_fn=<SumBackward0>),\n",
       " (2, 5): tensor(3.6183, grad_fn=<SumBackward0>),\n",
       " (28, 33): tensor(3.5804, grad_fn=<SumBackward0>),\n",
       " (20, 26): tensor(3.5563, grad_fn=<SumBackward0>),\n",
       " (0, 3): tensor(3.5290, grad_fn=<SumBackward0>),\n",
       " (14, 20): tensor(3.5158, grad_fn=<SumBackward0>),\n",
       " (31, 35): tensor(3.4577, grad_fn=<SumBackward0>),\n",
       " (30, 34): tensor(3.4394, grad_fn=<SumBackward0>),\n",
       " (18, 24): tensor(3.4348, grad_fn=<SumBackward0>),\n",
       " (9, 15): tensor(3.4295, grad_fn=<SumBackward0>),\n",
       " (27, 32): tensor(3.4128, grad_fn=<SumBackward0>),\n",
       " (29, 33): tensor(3.3868, grad_fn=<SumBackward0>),\n",
       " (25, 31): tensor(3.3167, grad_fn=<SumBackward0>),\n",
       " (20, 25): tensor(3.2522, grad_fn=<SumBackward0>),\n",
       " (2, 4): tensor(3.2445, grad_fn=<SumBackward0>),\n",
       " (8, 14): tensor(3.2434, grad_fn=<SumBackward0>),\n",
       " (17, 23): tensor(3.2428, grad_fn=<SumBackward0>),\n",
       " (19, 24): tensor(3.2348, grad_fn=<SumBackward0>),\n",
       " (28, 32): tensor(3.1728, grad_fn=<SumBackward0>),\n",
       " (11, 17): tensor(3.1685, grad_fn=<SumBackward0>),\n",
       " (13, 19): tensor(3.1676, grad_fn=<SumBackward0>),\n",
       " (10, 16): tensor(3.1641, grad_fn=<SumBackward0>),\n",
       " (1, 3): tensor(3.1546, grad_fn=<SumBackward0>),\n",
       " (31, 34): tensor(3.1463, grad_fn=<SumBackward0>),\n",
       " (12, 18): tensor(3.1151, grad_fn=<SumBackward0>),\n",
       " (3, 9): tensor(3.1110, grad_fn=<SumBackward0>),\n",
       " (30, 33): tensor(3.0533, grad_fn=<SumBackward0>),\n",
       " (32, 38): tensor(3.0463, grad_fn=<SumBackward0>),\n",
       " (29, 32): tensor(2.9793, grad_fn=<SumBackward0>),\n",
       " (0, 2): tensor(2.9734, grad_fn=<SumBackward0>),\n",
       " (18, 23): tensor(2.9555, grad_fn=<SumBackward0>),\n",
       " (21, 27): tensor(2.9196, grad_fn=<SumBackward0>),\n",
       " (12, 17): tensor(2.9152, grad_fn=<SumBackward0>),\n",
       " (26, 31): tensor(2.9144, grad_fn=<SumBackward0>),\n",
       " (9, 14): tensor(2.9089, grad_fn=<SumBackward0>),\n",
       " (15, 21): tensor(2.9033, grad_fn=<SumBackward0>),\n",
       " (10, 15): tensor(2.8829, grad_fn=<SumBackward0>),\n",
       " (11, 16): tensor(2.8812, grad_fn=<SumBackward0>),\n",
       " (20, 24): tensor(2.8499, grad_fn=<SumBackward0>),\n",
       " (36, 42): tensor(2.8496, grad_fn=<SumBackward0>),\n",
       " (4, 10): tensor(2.8382, grad_fn=<SumBackward0>),\n",
       " (16, 22): tensor(2.8167, grad_fn=<SumBackward0>),\n",
       " (13, 18): tensor(2.7826, grad_fn=<SumBackward0>),\n",
       " (22, 28): tensor(2.7605, grad_fn=<SumBackward0>),\n",
       " (31, 33): tensor(2.7602, grad_fn=<SumBackward0>),\n",
       " (19, 23): tensor(2.7556, grad_fn=<SumBackward0>),\n",
       " (2, 3): tensor(2.7448, grad_fn=<SumBackward0>),\n",
       " (7, 13): tensor(2.7110, grad_fn=<SumBackward0>),\n",
       " (21, 26): tensor(2.6795, grad_fn=<SumBackward0>),\n",
       " (23, 29): tensor(2.6601, grad_fn=<SumBackward0>),\n",
       " (30, 32): tensor(2.6457, grad_fn=<SumBackward0>),\n",
       " (14, 19): tensor(2.6390, grad_fn=<SumBackward0>),\n",
       " (12, 16): tensor(2.6279, grad_fn=<SumBackward0>),\n",
       " (27, 31): tensor(2.6103, grad_fn=<SumBackward0>),\n",
       " (11, 15): tensor(2.6001, grad_fn=<SumBackward0>),\n",
       " (1, 2): tensor(2.5990, grad_fn=<SumBackward0>),\n",
       " (5, 11): tensor(2.5918, grad_fn=<SumBackward0>),\n",
       " (32, 37): tensor(2.5852, grad_fn=<SumBackward0>),\n",
       " (13, 17): tensor(2.5827, grad_fn=<SumBackward0>),\n",
       " (22, 27): tensor(2.5669, grad_fn=<SumBackward0>),\n",
       " (3, 8): tensor(2.5644, grad_fn=<SumBackward0>),\n",
       " (4, 9): tensor(2.5554, grad_fn=<SumBackward0>),\n",
       " (15, 20): tensor(2.5507, grad_fn=<SumBackward0>),\n",
       " (6, 12): tensor(2.5506, grad_fn=<SumBackward0>),\n",
       " (17, 22): tensor(2.5355, grad_fn=<SumBackward0>),\n",
       " (37, 42): tensor(2.5350, grad_fn=<SumBackward0>),\n",
       " (33, 39): tensor(2.5089, grad_fn=<SumBackward0>),\n",
       " (34, 40): tensor(2.4531, grad_fn=<SumBackward0>),\n",
       " (35, 41): tensor(2.3968, grad_fn=<SumBackward0>),\n",
       " (16, 21): tensor(2.3827, grad_fn=<SumBackward0>),\n",
       " (21, 25): tensor(2.3754, grad_fn=<SumBackward0>),\n",
       " (20, 23): tensor(2.3707, grad_fn=<SumBackward0>),\n",
       " (28, 31): tensor(2.3703, grad_fn=<SumBackward0>),\n",
       " (10, 14): tensor(2.3623, grad_fn=<SumBackward0>),\n",
       " (31, 32): tensor(2.3527, grad_fn=<SumBackward0>),\n",
       " (12, 15): tensor(2.3468, grad_fn=<SumBackward0>),\n",
       " (5, 10): tensor(2.3385, grad_fn=<SumBackward0>),\n",
       " (22, 26): tensor(2.3269, grad_fn=<SumBackward0>),\n",
       " (23, 28): tensor(2.3265, grad_fn=<SumBackward0>),\n",
       " (13, 16): tensor(2.2954, grad_fn=<SumBackward0>),\n",
       " (8, 13): tensor(2.2782, grad_fn=<SumBackward0>),\n",
       " (14, 18): tensor(2.2541, grad_fn=<SumBackward0>),\n",
       " (18, 22): tensor(2.2482, grad_fn=<SumBackward0>),\n",
       " (24, 30): tensor(2.2458, grad_fn=<SumBackward0>),\n",
       " (33, 38): tensor(2.2437, grad_fn=<SumBackward0>),\n",
       " (3, 7): tensor(2.2299, grad_fn=<SumBackward0>),\n",
       " (32, 36): tensor(2.2221, grad_fn=<SumBackward0>),\n",
       " (6, 11): tensor(2.2181, grad_fn=<SumBackward0>),\n",
       " (2, 2): tensor(2.1892, grad_fn=<SumBackward0>),\n",
       " (7, 12): tensor(2.1825, grad_fn=<SumBackward0>),\n",
       " (29, 31): tensor(2.1767, grad_fn=<SumBackward0>),\n",
       " (38, 42): tensor(2.1720, grad_fn=<SumBackward0>),\n",
       " (23, 27): tensor(2.1330, grad_fn=<SumBackward0>),\n",
       " (17, 21): tensor(2.1016, grad_fn=<SumBackward0>),\n",
       " (34, 39): tensor(2.1014, grad_fn=<SumBackward0>),\n",
       " (36, 41): tensor(2.0853, grad_fn=<SumBackward0>),\n",
       " (11, 14): tensor(2.0795, grad_fn=<SumBackward0>),\n",
       " (35, 40): tensor(2.0670, grad_fn=<SumBackward0>),\n",
       " (5, 9): tensor(2.0557, grad_fn=<SumBackward0>),\n",
       " (14, 17): tensor(2.0542, grad_fn=<SumBackward0>),\n",
       " (19, 22): tensor(2.0483, grad_fn=<SumBackward0>),\n",
       " (16, 20): tensor(2.0301, grad_fn=<SumBackward0>),\n",
       " (22, 25): tensor(2.0228, grad_fn=<SumBackward0>),\n",
       " (13, 15): tensor(2.0143, grad_fn=<SumBackward0>),\n",
       " (4, 8): tensor(2.0088, grad_fn=<SumBackward0>),\n",
       " (21, 24): tensor(1.9731, grad_fn=<SumBackward0>),\n",
       " (6, 10): tensor(1.9647, grad_fn=<SumBackward0>),\n",
       " (24, 29): tensor(1.9528, grad_fn=<SumBackward0>),\n",
       " (9, 13): tensor(1.9438, grad_fn=<SumBackward0>),\n",
       " (32, 35): tensor(1.9076, grad_fn=<SumBackward0>),\n",
       " (23, 26): tensor(1.8930, grad_fn=<SumBackward0>),\n",
       " (7, 11): tensor(1.8500, grad_fn=<SumBackward0>),\n",
       " (30, 31): tensor(1.8432, grad_fn=<SumBackward0>),\n",
       " (34, 38): tensor(1.8362, grad_fn=<SumBackward0>),\n",
       " (12, 14): tensor(1.8262, grad_fn=<SumBackward0>),\n",
       " (18, 21): tensor(1.8143, grad_fn=<SumBackward0>),\n",
       " (3, 6): tensor(1.7972, grad_fn=<SumBackward0>),\n",
       " (33, 37): tensor(1.7826, grad_fn=<SumBackward0>),\n",
       " (37, 41): tensor(1.7708, grad_fn=<SumBackward0>),\n",
       " (14, 16): tensor(1.7669, grad_fn=<SumBackward0>),\n",
       " (25, 30): tensor(1.7666, grad_fn=<SumBackward0>),\n",
       " (36, 40): tensor(1.7556, grad_fn=<SumBackward0>),\n",
       " (8, 12): tensor(1.7497, grad_fn=<SumBackward0>),\n",
       " (17, 20): tensor(1.7489, grad_fn=<SumBackward0>),\n",
       " (35, 39): tensor(1.7153, grad_fn=<SumBackward0>),\n",
       " (39, 42): tensor(1.7109, grad_fn=<SumBackward0>),\n",
       " (6, 9): tensor(1.6819, grad_fn=<SumBackward0>),\n",
       " (4, 7): tensor(1.6743, grad_fn=<SumBackward0>),\n",
       " (15, 19): tensor(1.6739, grad_fn=<SumBackward0>),\n",
       " (20, 22): tensor(1.6634, grad_fn=<SumBackward0>),\n",
       " (22, 24): tensor(1.6205, grad_fn=<SumBackward0>),\n",
       " (24, 28): tensor(1.6192, grad_fn=<SumBackward0>),\n",
       " (19, 21): tensor(1.6144, grad_fn=<SumBackward0>),\n",
       " (7, 10): tensor(1.5966, grad_fn=<SumBackward0>),\n",
       " (32, 34): tensor(1.5962, grad_fn=<SumBackward0>),\n",
       " (23, 25): tensor(1.5888, grad_fn=<SumBackward0>),\n",
       " (31, 31): tensor(1.5502, grad_fn=<SumBackward0>),\n",
       " (5, 8): tensor(1.5091, grad_fn=<SumBackward0>),\n",
       " (21, 23): tensor(1.4939, grad_fn=<SumBackward0>),\n",
       " (13, 14): tensor(1.4937, grad_fn=<SumBackward0>),\n",
       " (14, 15): tensor(1.4857, grad_fn=<SumBackward0>),\n",
       " (25, 29): tensor(1.4735, grad_fn=<SumBackward0>),\n",
       " (18, 20): tensor(1.4616, grad_fn=<SumBackward0>),\n",
       " (35, 38): tensor(1.4501, grad_fn=<SumBackward0>),\n",
       " (40, 42): tensor(1.4457, grad_fn=<SumBackward0>),\n",
       " (37, 40): tensor(1.4410, grad_fn=<SumBackward0>),\n",
       " (3, 5): tensor(1.4291, grad_fn=<SumBackward0>),\n",
       " (24, 27): tensor(1.4257, grad_fn=<SumBackward0>),\n",
       " (33, 36): tensor(1.4196, grad_fn=<SumBackward0>),\n",
       " (8, 11): tensor(1.4172, grad_fn=<SumBackward0>),\n",
       " (9, 12): tensor(1.4152, grad_fn=<SumBackward0>),\n",
       " (38, 41): tensor(1.4078, grad_fn=<SumBackward0>),\n",
       " (36, 39): tensor(1.4039, grad_fn=<SumBackward0>),\n",
       " (10, 13): tensor(1.3972, grad_fn=<SumBackward0>),\n",
       " (34, 37): tensor(1.3751, grad_fn=<SumBackward0>),\n",
       " (26, 30): tensor(1.3643, grad_fn=<SumBackward0>),\n",
       " (7, 9): tensor(1.3138, grad_fn=<SumBackward0>),\n",
       " (15, 18): tensor(1.2890, grad_fn=<SumBackward0>),\n",
       " (19, 20): tensor(1.2617, grad_fn=<SumBackward0>),\n",
       " (4, 6): tensor(1.2416, grad_fn=<SumBackward0>),\n",
       " (20, 21): tensor(1.2294, grad_fn=<SumBackward0>),\n",
       " (32, 33): tensor(1.2101, grad_fn=<SumBackward0>),\n",
       " (23, 24): tensor(1.1866, grad_fn=<SumBackward0>),\n",
       " (24, 26): tensor(1.1857, grad_fn=<SumBackward0>),\n",
       " (5, 7): tensor(1.1746, grad_fn=<SumBackward0>),\n",
       " (8, 10): tensor(1.1639, grad_fn=<SumBackward0>),\n",
       " (16, 19): tensor(1.1533, grad_fn=<SumBackward0>),\n",
       " (22, 23): tensor(1.1412, grad_fn=<SumBackward0>),\n",
       " (25, 28): tensor(1.1400, grad_fn=<SumBackward0>),\n",
       " (36, 38): tensor(1.1387, grad_fn=<SumBackward0>),\n",
       " (6, 8): tensor(1.1353, grad_fn=<SumBackward0>),\n",
       " (11, 13): tensor(1.1144, grad_fn=<SumBackward0>),\n",
       " (33, 35): tensor(1.1051, grad_fn=<SumBackward0>),\n",
       " (41, 42): tensor(1.0940, grad_fn=<SumBackward0>),\n",
       " (37, 39): tensor(1.0893, grad_fn=<SumBackward0>),\n",
       " (15, 17): tensor(1.0891, grad_fn=<SumBackward0>),\n",
       " (9, 11): tensor(1.0827, grad_fn=<SumBackward0>),\n",
       " (38, 40): tensor(1.0780, grad_fn=<SumBackward0>),\n",
       " (26, 29): tensor(1.0712, grad_fn=<SumBackward0>),\n",
       " (27, 30): tensor(1.0602, grad_fn=<SumBackward0>),\n",
       " (3, 4): tensor(1.0553, grad_fn=<SumBackward0>),\n",
       " (34, 36): tensor(1.0121, grad_fn=<SumBackward0>),\n",
       " (35, 37): tensor(0.9890, grad_fn=<SumBackward0>),\n",
       " (14, 14): tensor(0.9651, grad_fn=<SumBackward0>),\n",
       " (39, 41): tensor(0.9467, grad_fn=<SumBackward0>),\n",
       " (25, 27): tensor(0.9464, grad_fn=<SumBackward0>),\n",
       " (24, 25): tensor(0.8816, grad_fn=<SumBackward0>),\n",
       " (8, 9): tensor(0.8811, grad_fn=<SumBackward0>),\n",
       " (20, 20): tensor(0.8768, grad_fn=<SumBackward0>),\n",
       " (4, 5): tensor(0.8735, grad_fn=<SumBackward0>),\n",
       " (17, 19): tensor(0.8721, grad_fn=<SumBackward0>),\n",
       " (10, 12): tensor(0.8686, grad_fn=<SumBackward0>),\n",
       " (12, 13): tensor(0.8610, grad_fn=<SumBackward0>),\n",
       " (9, 10): tensor(0.8294, grad_fn=<SumBackward0>),\n",
       " (37, 38): tensor(0.8241, grad_fn=<SumBackward0>),\n",
       " (28, 30): tensor(0.8201, grad_fn=<SumBackward0>),\n",
       " (32, 32): tensor(0.8025, grad_fn=<SumBackward0>),\n",
       " (15, 16): tensor(0.8017, grad_fn=<SumBackward0>),\n",
       " (6, 7): tensor(0.8009, grad_fn=<SumBackward0>),\n",
       " (33, 34): tensor(0.7936, grad_fn=<SumBackward0>),\n",
       " (21, 22): tensor(0.7866, grad_fn=<SumBackward0>),\n",
       " (0, 1): tensor(0.7841, grad_fn=<SumBackward0>),\n",
       " (16, 18): tensor(0.7684, grad_fn=<SumBackward0>),\n",
       " (7, 8): tensor(0.7672, grad_fn=<SumBackward0>),\n",
       " (27, 29): tensor(0.7671, grad_fn=<SumBackward0>),\n",
       " (42, 42): tensor(0.7643, grad_fn=<SumBackward0>),\n",
       " (5, 6): tensor(0.7419, grad_fn=<SumBackward0>),\n",
       " (26, 28): tensor(0.7377, grad_fn=<SumBackward0>),\n",
       " (38, 39): tensor(0.7263, grad_fn=<SumBackward0>),\n",
       " (23, 23): tensor(0.7073, grad_fn=<SumBackward0>),\n",
       " (25, 26): tensor(0.7064, grad_fn=<SumBackward0>),\n",
       " (34, 35): tensor(0.6975, grad_fn=<SumBackward0>),\n",
       " (40, 41): tensor(0.6815, grad_fn=<SumBackward0>),\n",
       " (36, 37): tensor(0.6776, grad_fn=<SumBackward0>),\n",
       " (29, 30): tensor(0.6266, grad_fn=<SumBackward0>),\n",
       " (35, 36): tensor(0.6260, grad_fn=<SumBackward0>),\n",
       " (39, 40): tensor(0.6169, grad_fn=<SumBackward0>),\n",
       " (11, 12): tensor(0.5858, grad_fn=<SumBackward0>),\n",
       " (18, 19): tensor(0.5848, grad_fn=<SumBackward0>),\n",
       " (16, 17): tensor(0.5685, grad_fn=<SumBackward0>),\n",
       " (3, 3): tensor(0.5556, grad_fn=<SumBackward0>),\n",
       " (9, 9): tensor(0.5466, grad_fn=<SumBackward0>),\n",
       " (26, 27): tensor(0.5442, grad_fn=<SumBackward0>),\n",
       " (10, 11): tensor(0.5361, grad_fn=<SumBackward0>),\n",
       " (13, 13): tensor(0.5285, grad_fn=<SumBackward0>),\n",
       " (28, 29): tensor(0.5271, grad_fn=<SumBackward0>),\n",
       " (15, 15): tensor(0.5206, grad_fn=<SumBackward0>),\n",
       " (4, 4): tensor(0.4997, grad_fn=<SumBackward0>),\n",
       " (17, 18): tensor(0.4872, grad_fn=<SumBackward0>),\n",
       " (24, 24): tensor(0.4793, grad_fn=<SumBackward0>),\n",
       " (38, 38): tensor(0.4611, grad_fn=<SumBackward0>),\n",
       " (22, 22): tensor(0.4339, grad_fn=<SumBackward0>),\n",
       " (27, 28): tensor(0.4336, grad_fn=<SumBackward0>),\n",
       " (7, 7): tensor(0.4328, grad_fn=<SumBackward0>),\n",
       " (1, 1): tensor(0.4098, grad_fn=<SumBackward0>),\n",
       " (33, 33): tensor(0.4076, grad_fn=<SumBackward0>),\n",
       " (25, 25): tensor(0.4023, grad_fn=<SumBackward0>),\n",
       " (34, 34): tensor(0.3861, grad_fn=<SumBackward0>),\n",
       " (19, 19): tensor(0.3849, grad_fn=<SumBackward0>),\n",
       " (0, 0): tensor(0.3744, grad_fn=<SumBackward0>),\n",
       " (5, 5): tensor(0.3738, grad_fn=<SumBackward0>),\n",
       " (6, 6): tensor(0.3681, grad_fn=<SumBackward0>),\n",
       " (37, 37): tensor(0.3630, grad_fn=<SumBackward0>),\n",
       " (21, 21): tensor(0.3526, grad_fn=<SumBackward0>),\n",
       " (40, 40): tensor(0.3517, grad_fn=<SumBackward0>),\n",
       " (8, 8): tensor(0.3345, grad_fn=<SumBackward0>),\n",
       " (29, 29): tensor(0.3335, grad_fn=<SumBackward0>),\n",
       " (12, 12): tensor(0.3325, grad_fn=<SumBackward0>),\n",
       " (41, 41): tensor(0.3298, grad_fn=<SumBackward0>),\n",
       " (36, 36): tensor(0.3146, grad_fn=<SumBackward0>),\n",
       " (35, 35): tensor(0.3114, grad_fn=<SumBackward0>),\n",
       " (26, 26): tensor(0.3041, grad_fn=<SumBackward0>),\n",
       " (30, 30): tensor(0.2930, grad_fn=<SumBackward0>),\n",
       " (17, 17): tensor(0.2873, grad_fn=<SumBackward0>),\n",
       " (10, 10): tensor(0.2828, grad_fn=<SumBackward0>),\n",
       " (16, 16): tensor(0.2812, grad_fn=<SumBackward0>),\n",
       " (39, 39): tensor(0.2652, grad_fn=<SumBackward0>),\n",
       " (11, 11): tensor(0.2533, grad_fn=<SumBackward0>),\n",
       " (27, 27): tensor(0.2400, grad_fn=<SumBackward0>),\n",
       " (18, 18): tensor(0.1999, grad_fn=<SumBackward0>),\n",
       " (28, 28): tensor(0.1935, grad_fn=<SumBackward0>)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_span_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy line to doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEMP.Mana, a Chinese cyber espionage group'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Doc_Tokens:\n",
    "    def __init__(self, doc, fullword_tokens, subword_tokens, subword_idx):\n",
    "        self.doc = doc\n",
    "        self.fullword_tokens = fullword_tokens\n",
    "        self.subword_tokens = subword_tokens\n",
    "        self.subword_idx = subword_idx\n",
    "\n",
    "    def __getitem__(self, val):\n",
    "        \"\"\"\n",
    "        Slice the doc. This is wrt to AllenNLP's inclusive spans.\n",
    "        Meaning that it is not compatible with Spacy's exclusive spans\n",
    "        \n",
    "        For example, the input doc[0:4] in allennlp's inclusive span \n",
    "        will return the equivalent of self.doc[0:5] in spacy's terms\n",
    "\n",
    "        reference for __getitem__:\n",
    "        https://stackoverflow.com/questions/2936863/implementing-slicing-in-getitem\n",
    "        \"\"\"\n",
    "        if isinstance(val, slice):\n",
    "            # +1 because allennlp spans are inclusive\n",
    "            return self.doc[val.start: val.stop+1]\n",
    "            if val.stop == val.start:\n",
    "                return self.doc[val.start: val.stop+1]\n",
    "            else:\n",
    "                return self.doc[val.start: val.stop]\n",
    "        else:\n",
    "            return self.doc[val]\n",
    "        \n",
    "class Doc_Span(Doc_Tokens):\n",
    "    def __init__(self, doc_tokens:Doc_Tokens, span_tuple:tuple):\n",
    "        self.doc = doc_tokens.doc\n",
    "        self.fullword_tokens = doc_tokens.fullword_tokens\n",
    "        self.subword_tokens = doc_tokens.subword_tokens\n",
    "        self.subword_idx = doc_tokens.subword_idx\n",
    "        self.span_tuple = span_tuple\n",
    "        self.span_text = self.doc[self.span_tuple[0]:self.span_tuple[1]+1].text\n",
    "\n",
    "doc_span = Doc_Span(doc, (2,8))\n",
    "doc_span.span_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believe TEMP.Mana, a Chinese cyber espionage'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_span = Doc_Span(doc, (1,7))\n",
    "doc_span.span_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparta loss\n",
    "input: Query, reference, Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 470 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def brute_sum_tensors(list_of_tensors):\n",
    "    return torch.sum( torch.stack( list_of_tensors ) )\n",
    "\n",
    "def xentropy_l2r(model, query_span: Doc_Span, references: list, negatives: list):\n",
    "    \"\"\"\n",
    "    Cross-entropy learning to rank loss as defined in:\n",
    "    https://arxiv.org/pdf/2009.13013.pdf\n",
    "    \n",
    "    A stronger reference for this loss is eq (3) in:\n",
    "    https://papers.nips.cc/paper/2009/file/2f55707d4193dc27118a0f19a1985716-Paper.pdf\n",
    "    \n",
    "    # TODO: this should get batched\n",
    "    \"\"\"\n",
    "    # perform encoding\n",
    "    query_encoding = model.encode_span(query_span, query_span.span_tuple)\n",
    "    reference_encodings = [model.encode_span(reference_span, reference_span.span_tuple) \n",
    "                           for reference_span in references]  if type(references[0]) != torch.Tensor else references\n",
    "    negative_encodings = [model.encode_span(neg_span, neg_span.span_tuple) \n",
    "                         for neg_span in negatives] if type(negatives[0]) != torch.Tensor else negatives\n",
    "\n",
    "    # calc loss\n",
    "    pos_sim = [model.encoding_sim_score(query_encoding, reference_encoding) \n",
    "               for reference_encoding in reference_encodings]\n",
    "    neg_sim = [torch.exp( model.encoding_sim_score(query_encoding, negative_encoding) ) \n",
    "               for negative_encoding in negative_encodings]\n",
    "    loss = torch.log( brute_sum_tensors(neg_sim) ) - brute_sum_tensors(pos_sim)\n",
    "    return loss\n",
    "\n",
    "%time loss = xentropy_l2r(model, Doc_Span(doc, (2,8)), [Doc_Span(ref_doc, (1,2))]*2 ,[Doc_Span(doc, (1,7))]*2 )\n",
    "# loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra document loss\n",
    "1 query and label against support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APT10 group\n",
      "TEMP.Mana, a Chinese cyber espionage group\n"
     ]
    }
   ],
   "source": [
    "references_spans = [Doc_Span( handler.process_sentence(ref_text), (1,2)) ]\n",
    "answer_spans = [Doc_Span(doc, (2,8))]\n",
    "print(references_spans[0].span_text)\n",
    "print(answer_spans[0].span_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0098, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def intra_doc_loss(answer_spans: list, references_spans: list):\n",
    "    \n",
    "    # genereate correct encodings\n",
    "    answer_spans_encodings = model.encode_spans( answer_spans[0], [answer_span.span_tuple for answer_span in answer_spans] )\n",
    "    \n",
    "    # generate wrong spans\n",
    "    all_possible_spans = enumerate_spans( answer_spans[0].doc, max_span_width = 7)\n",
    "    all_wrong_spans = [span for span in all_possible_spans \n",
    "                       if span not in [answer_span.span_tuple for answer_span in answer_spans]]\n",
    "    all_wrong_span_encodings = model.encode_spans( answer_spans[0], all_wrong_spans )\n",
    "    \n",
    "    # iterate through references_spans & sum loss\n",
    "    losses = []\n",
    "    for reference in references_spans:\n",
    "        loss_ = xentropy_l2r(model, reference, answer_spans_encodings, all_wrong_span_encodings)\n",
    "        losses.append(loss_)\n",
    "        \n",
    "    return brute_sum_tensors(losses)\n",
    "\n",
    "intra_doc_loss(answer_spans, references_spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interdocument loss\n",
    "support against support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "query = np.random.choice(references_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = [references_span for references_span in references_spans if references_span != query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
